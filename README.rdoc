= localize.me

<b>Localize Me</b> is a solution for low cost indoor localization.


= Visualization solution

Naturally we all emit energy, this energy comes in different forms. The energy we emit is 
usually the result of the completion of a task. A task can be anything, even sitting and 
typing is certain task that emits energy. We radiate waves as we move, we project a mechanical 
energy when we speak. All things that leave a sort of trail that could be thought of like a 
contrail from an airplane. Except invisible. 

What would it be like to actually see the energy we emit? Without using IR technologies we 
can look to tracing our movements and estimate what these trails could look like.

The visualization is realized using the latest technologies like SVG, Canvas, HTML5, CSS3 
and Javascript and we are willing to see more and more different visualization out there
pretty soon.


= Hardware solution

Localiz Me was called to life by the need to track people position in indoor locations, 
defining a low cost and easy to install solution. Nowadays the market offers different 
solutions based on technologies like UWB, Bluetooth, WiFi and Zigbbe. 

They all work out, but noone was  cheap and detailed ad the same time. For this reason, 
after experimenting different solutions few weeks, we finally decided to use wifi cameras. 
You can have them at reasonable prices and out of some setups, everything just works.

To elaborate the camera signal we have used {openTSPS}[https://github.com/labatrockwell/openTSPS].
It is an open-source software that can be configured through 5 tabs. Each of them has
some useful parameters that we are gonna set in different ways from installation to 
installation.

* Video Tab -> Use amplification. Gives an higher contrast to the video (31 is a good value).
* Background -> Progressive background. Set it to the max values, as the frame rate we sample is 
very low for performance reasons. It rebuild the background. This is very useful if you move an 
object because in this way after a while it disappears.
* Differencing -> Threshold. A good value is around 20. If you need to check small staff you 
need to lower it down. If you need to track big objects you can put a bigger value.
* Differencing -> Type of differencing. Depending from the background use the best one. If you
have a white floor, use dark on light (recognize dark stuff on the white background)
* Differencing -> Use highpass. It remove some pixel noise. A good value is around 100 for 
the blur and 15 for the noise.
* Differencing -> Use shape smoothing. It makes the blob much more cleaner. a good value is around 7.
* Sensing. Here ignore nested blobs and deactivate everything else.
* Communication. Keep activated only Send via Websockets and remember to set the right port.


= Architecture

One goal we wanted to reach was realtime comunication with the visualization sistem you
will have in the browser. To make this we used NodeJS and WebSockets. What we do is to 
connect to the data offered from the cams, collect and normalize them all, and then offer
them through websockets. 

We have also a virtual setup so that you can expetiment new visual solutions in no time.
Here a short guide to make the server up and running.

  $ monogd # you need MongoDB
  $ coffee app.coffee

If you want to simulate the system start n virtual cameras sending a port as option

  $ node camera
  $ node camera -p 4002
  $ node camera -p 4003 -m 
  $ node camera -p 4004 -m 

Open your app at localhost:4000 you'll see you application up and running.


